#!/usr/bin/env python3
"""
Comprehensive Phase Evaluation Results Analyzer
Analyzes results from any phase and provides detailed rankings
"""

import json
import os
import sys
from typing import Dict, List, Any, Optional
from datetime import datetime

def load_phase_results(phase_name: str) -> Dict[str, Any]:
    """Load results from a specific phase"""
    results_file = f"output/{phase_name}/comprehensive_results.json"
    
    if not os.path.exists(results_file):
        print(f"‚ùå Results file not found: {results_file}")
        print(f"Available phases:")
        for item in os.listdir("output"):
            if os.path.isdir(os.path.join("output", item)):
                print(f"  - {item}")
        return {}
    
    try:
        with open(results_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå Error loading results: {e}")
        return {}

def analyze_phase_results(results: Dict[str, Any], phase_name: str):
    """Analyze and rank models for a specific phase"""
    
    
    successful_models = {}
    for model_name, result in results.items():
        if result.get("success", False):
            successful_models[model_name] = result
    
    print(f"üìä Analyzing {len(successful_models)} successful models out of {len(results)} total")
    print(f"üéØ Phase: {phase_name.upper()}")
    print("=" * 80)
    
    if not successful_models:
        print("‚ùå No successful models found!")
        return
    
    
    if "topic_clustering" in phase_name:
        analyze_clustering_results(successful_models)
    elif "summarization" in phase_name:
        analyze_summarization_results(successful_models)
    elif "extraction" in phase_name:
        analyze_extraction_results(successful_models)
    else:
        analyze_general_results(successful_models)
    
    
    analyze_general_metrics(successful_models)

def analyze_clustering_results(successful_models: Dict[str, Any]):
    """Analyze topic clustering specific metrics"""
    
    
    print("\nüèÜ **BEST COVERAGE** (Percentage of messages clustered)")
    coverage_ranked = sorted(successful_models.items(), 
                            key=lambda x: x[1]["metrics"]["coverage"], reverse=True)
    for i, (model, result) in enumerate(coverage_ranked[:5]):
        coverage = result["metrics"]["coverage"] * 100
        print(f"{i+1}. {model}: {coverage:.1f}%")
    
    
    print("\nüèÜ **BEST THREAD COHERENCE** (How well threads are preserved)")
    coherence_ranked = sorted(successful_models.items(), 
                             key=lambda x: x[1]["metrics"]["thread_coherence"], reverse=True)
    for i, (model, result) in enumerate(coherence_ranked[:5]):
        coherence = result["metrics"]["thread_coherence"] * 100
        print(f"{i+1}. {model}: {coherence:.1f}%")
    
    
    print("\nüèÜ **BEST CLUSTER QUALITY** (Optimal number of clusters)")
    cluster_quality = []
    for model, result in successful_models.items():
        num_clusters = result["metrics"]["num_clusters"]
        
        quality_score = 1.0 - abs(num_clusters - 12.5) / 12.5  
        cluster_quality.append((model, quality_score, num_clusters))
    
    cluster_quality.sort(key=lambda x: x[1], reverse=True)
    for i, (model, score, num_clusters) in enumerate(cluster_quality[:5]):
        print(f"{i+1}. {model}: {num_clusters} clusters (quality: {score:.3f})")

def analyze_summarization_results(successful_models: Dict[str, Any]):
    """Analyze summarization specific metrics"""
    
    
    if "summary_quality" in successful_models[list(successful_models.keys())[0]]["metrics"]:
        print("\nüèÜ **BEST SUMMARY QUALITY**")
        quality_ranked = sorted(successful_models.items(), 
                               key=lambda x: x[1]["metrics"].get("summary_quality", 0), reverse=True)
        for i, (model, result) in enumerate(quality_ranked[:5]):
            quality = result["metrics"].get("summary_quality", 0)
            print(f"{i+1}. {model}: {quality:.3f}")
    
    
    if "summary_length" in successful_models[list(successful_models.keys())[0]]["metrics"]:
        print("\nüèÜ **OPTIMAL SUMMARY LENGTH**")
        length_ranked = sorted(successful_models.items(), 
                              key=lambda x: abs(x[1]["metrics"].get("summary_length", 0) - 200))  
        for i, (model, result) in enumerate(length_ranked[:5]):
            length = result["metrics"].get("summary_length", 0)
            print(f"{i+1}. {model}: {length} words")

def analyze_extraction_results(successful_models: Dict[str, Any]):
    """Analyze information extraction specific metrics"""
    
    
    if "extraction_accuracy" in successful_models[list(successful_models.keys())[0]]["metrics"]:
        print("\nüèÜ **BEST EXTRACTION ACCURACY**")
        accuracy_ranked = sorted(successful_models.items(), 
                                key=lambda x: x[1]["metrics"].get("extraction_accuracy", 0), reverse=True)
        for i, (model, result) in enumerate(accuracy_ranked[:5]):
            accuracy = result["metrics"].get("extraction_accuracy", 0) * 100
            print(f"{i+1}. {model}: {accuracy:.1f}%")
    
    
    if "entities_extracted" in successful_models[list(successful_models.keys())[0]]["metrics"]:
        print("\nüèÜ **MOST ENTITIES EXTRACTED**")
        entities_ranked = sorted(successful_models.items(), 
                                key=lambda x: x[1]["metrics"].get("entities_extracted", 0), reverse=True)
        for i, (model, result) in enumerate(entities_ranked[:5]):
            entities = result["metrics"].get("entities_extracted", 0)
            print(f"{i+1}. {model}: {entities} entities")

def analyze_general_results(successful_models: Dict[str, Any]):
    """Analyze general metrics for any phase"""
    print("\nüèÜ **GENERAL PERFORMANCE METRICS**")
    
    
    sample_model = list(successful_models.values())[0]
    available_metrics = list(sample_model["metrics"].keys())
    print(f"Available metrics: {', '.join(available_metrics)}")

def analyze_general_metrics(successful_models: Dict[str, Any]):
    """Analyze general performance metrics for all phases"""
    
    
    print("\nüèÜ **MOST COST EFFICIENT** (Lowest cost per evaluation)")
    cost_ranked = sorted(successful_models.items(), 
                        key=lambda x: x[1]["cost"]["total_cost"])
    for i, (model, result) in enumerate(cost_ranked[:5]):
        cost = result["cost"]["total_cost"]
        print(f"{i+1}. {model}: ${cost:.6f}")
    
    
    print("\nüèÜ **FASTEST** (Lowest response time)")
    speed_ranked = sorted(successful_models.items(), 
                         key=lambda x: x[1]["duration"])
    for i, (model, result) in enumerate(speed_ranked[:5]):
        duration = result["duration"]
        print(f"{i+1}. {model}: {duration:.2f}s")
    
    
    print("\nüèÜ **MOST TOKEN EFFICIENT** (Lowest tokens per evaluation)")
    token_ranked = sorted(successful_models.items(), 
                         key=lambda x: x[1]["usage"]["total_tokens"])
    for i, (model, result) in enumerate(token_ranked[:5]):
        tokens = result["usage"]["total_tokens"]
        print(f"{i+1}. {model}: {tokens:,} tokens")
    
    
    print("\nüèÜ **OVERALL BEST** (Combined score: performance - normalized cost - normalized time)")
    overall_scores = {}
    for model, result in successful_models.items():
        
        metrics = result["metrics"]
        if "coverage" in metrics:
            performance = metrics["coverage"] + metrics.get("thread_coherence", 0)
        elif "summary_quality" in metrics:
            performance = metrics["summary_quality"]
        elif "extraction_accuracy" in metrics:
            performance = metrics["extraction_accuracy"]
        else:
            
            performance = list(metrics.values())[0] if metrics else 0
        
        cost = result["cost"]["total_cost"]
        duration = result["duration"]
        
        
        max_cost = max(r["cost"]["total_cost"] for r in successful_models.values())
        max_duration = max(r["duration"] for r in successful_models.values())
        
        normalized_cost = cost / max_cost if max_cost > 0 else 0
        normalized_duration = duration / max_duration if max_duration > 0 else 0
        
        
        overall_score = performance - normalized_cost - normalized_duration
        overall_scores[model] = overall_score
    
    overall_ranked = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
    for i, (model, score) in enumerate(overall_ranked[:10]):
        result = successful_models[model]
        cost = result["cost"]["total_cost"]
        duration = result["duration"]
        tokens = result["usage"]["total_tokens"]
        print(f"{i+1}. {model}: Score {score:.3f} (Cost: ${cost:.6f}, Time: {duration:.2f}s, Tokens: {tokens:,})")
    
    
    print("\nüèÜ **BEST BY PROVIDER**")
    providers = {}
    for model, result in successful_models.items():
        provider = result["provider"]
        if provider not in providers:
            providers[provider] = []
        providers[provider].append((model, overall_scores[model]))
    
    for provider, models in providers.items():
        best_model = max(models, key=lambda x: x[1])
        print(f"{provider.upper()}: {best_model[0]} (Score: {best_model[1]:.3f})")
    
    
    print("\nüîç **DETAILED ANALYSIS OF TOP 3 MODELS**")
    for i, (model, score) in enumerate(overall_ranked[:3]):
        result = successful_models[model]
        print(f"\n{i+1}. {model}")
        print(f"   Provider: {result['provider']}")
        print(f"   Cost: ${result['cost']['total_cost']:.6f}")
        print(f"   Duration: {result['duration']:.2f}s")
        print(f"   Tokens: {result['usage']['total_tokens']:,}")
        print(f"   Metrics: {result['metrics']}")

def list_available_phases():
    """List all available phases with results"""
    print("üìÅ Available phases with results:")
    print("=" * 40)
    
    if not os.path.exists("output"):
        print("‚ùå No output directory found")
        return []
    
    phases = []
    for item in os.listdir("output"):
        phase_dir = os.path.join("output", item)
        if os.path.isdir(phase_dir):
            results_file = os.path.join(phase_dir, "comprehensive_results.json")
            if os.path.exists(results_file):
                phases.append(item)
                print(f"‚úÖ {item}")
            else:
                print(f"‚ö†Ô∏è  {item} (no results file)")
    
    return phases

def main():
    """Main function"""
    print("üéØ Phase Evaluation Results Analyzer")
    print("=" * 50)
    
    
    available_phases = list_available_phases()
    
    if not available_phases:
        print("\n‚ùå No phases with results found!")
        return
    
    
    if len(sys.argv) > 1:
        phase_name = sys.argv[1]
        if phase_name not in available_phases:
            print(f"‚ùå Phase '{phase_name}' not found in available phases: {available_phases}")
            return
    else:
        
        phase_name = available_phases[0]
        print(f"\nüìä Analyzing phase: {phase_name}")
    
    
    results = load_phase_results(phase_name)
    if results:
        analyze_phase_results(results, phase_name)
    
    print(f"\n‚úÖ Analysis complete for {phase_name}")

if __name__ == "__main__":
    main()
