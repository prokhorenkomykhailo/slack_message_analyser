# üöÄ Quick Formula Reference Card

## **For Client Verification - All Formulas at a Glance**

---

## üìä **Core Metrics (Row-Level)**

| Metric | Formula | Example |
|--------|---------|---------|
| **Deviation %** | `((LLM_COUNT - BENCHMARK_COUNT) / BENCHMARK_COUNT) √ó 100` | `((60-47)/47) √ó 100 = 27.66%` |
| **Coverage %** | `(MATCHED_MESSAGES / BENCHMARK_COUNT) √ó 100` | `(47/47) √ó 100 = 100%` |
| **Precision %** | `(MATCHED_MESSAGES / LLM_COUNT) √ó 100` | `(47/60) √ó 100 = 78.33%` |
| **Recall %** | `(MATCHED_MESSAGES / BENCHMARK_COUNT) √ó 100` | `(47/47) √ó 100 = 100%` |

---

## üèÜ **Scoring Formulas**

### **Final Score (Simple)**
```
FINAL_SCORE = (Deviation_Score √ó 0.4) + (Coverage √ó 0.3) + (Precision √ó 0.3)

Where: Deviation_Score = max(0, 100 - |Deviation%|)
```

### **Improved Model Score (Advanced)**
```
IMPROVED_SCORE = (Cluster_Count √ó 0.25) + (Coverage √ó 0.25) + (Precision √ó 0.25) + (Deviation √ó 0.25)

Where:
- Cluster_Count = min(expected, actual) / max(expected, actual) √ó 100
- Coverage = found_clusters / expected_clusters √ó 100  
- Precision = average(precision across all clusters)
- Deviation = max(0, 100 - average(|deviation%|))
```

---

## üí∞ **Cost Formula**
```
COST = (INPUT_TOKENS/1000 √ó INPUT_RATE) + (OUTPUT_TOKENS/1000 √ó OUTPUT_RATE)
```

---

## üîç **How to Verify**

1. **Pick any row** from `llm_analysis_with_improved_scores.csv`
2. **Use formulas above** to recalculate each metric
3. **Compare results** with CSV values
4. **Run verification script**: `python verify_calculations.py`

---

## üìÅ **Source Files**
- **Results**: `llm_analysis_with_improved_scores.csv`
- **Benchmark**: `phases/phase3_clusters.json`
- **Scripts**: `improved_model_scoring.py`, `add_final_score.py`
- **Full Docs**: `FORMULA_DOCUMENTATION.md`

**‚úÖ Every calculation is transparent and verifiable!**

