#!/usr/bin/env python3
"""
Cohere model with properly fixed attention mask
This version fixes the pad token issue correctly
"""

import os
import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

def fix_tokenizer_attention_mask():
    """Fix the tokenizer attention mask issue properly"""
    
    print("ğŸ”§ Fixing Cohere Tokenizer Attention Mask")
    print("=" * 45)
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained("CohereLabs/c4ai-command-r-plus-08-2024")
    
    print(f"Original pad_token: {tokenizer.pad_token}")
    print(f"Original eos_token: {tokenizer.eos_token}")
    
    # Fix 1: Set a different pad token
    if tokenizer.pad_token is None:
        # Option A: Use unk_token as pad_token (recommended)
        tokenizer.pad_token = tokenizer.unk_token
        print("âœ… Set pad_token = unk_token")
        
        # Option B: Add a new special token (alternative)
        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        # print("âœ… Added new pad_token: [PAD]")
    
    print(f"Fixed pad_token: {tokenizer.pad_token}")
    print(f"Fixed eos_token: {tokenizer.eos_token}")
    
    # Verify they're different
    if tokenizer.pad_token != tokenizer.eos_token:
        print("âœ… pad_token and eos_token are now different")
    else:
        print("âŒ pad_token and eos_token are still the same")
    
    return tokenizer

def test_fixed_generation():
    """Test generation with fixed attention mask"""
    
    print("\\nğŸ§ª Testing Generation with Fixed Attention Mask")
    print("=" * 50)
    
    try:
        # Fix tokenizer
        tokenizer = fix_tokenizer_attention_mask()
        
        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            "CohereLabs/c4ai-command-r-plus-08-2024",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… Model loaded")
        
        # Test simple generation
        test_prompt = "Hello, how are you?"
        messages = [{"role": "user", "content": test_prompt}]
        
        # Apply chat template
        inputs = tokenizer.apply_chat_template(
            messages, 
            tokenize=True, 
            add_generation_prompt=True, 
            return_tensors="pt"
        )
        
        print(f"âœ… Input prepared: {inputs.shape}")
        
        # Generate with proper attention mask
        print("ğŸ”„ Generating...")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.3,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        
        print("âœ… Generation completed!")
        print(f"Response: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        return False

def test_clustering_with_fixed_attention():
    """Test clustering with fixed attention mask"""
    
    print("\\nğŸš€ Testing Step 1 Clustering with Fixed Attention")
    print("=" * 55)
    
    try:
        # Load messages
        messages_file = "data/Synthetic_Slack_Messages.csv"
        if not os.path.exists(messages_file):
            print(f"âŒ File not found: {messages_file}")
            return False
        
        df = pd.read_csv(messages_file)
        test_messages = df.head(5)  # Just 5 messages for testing
        
        print(f"âœ… Testing with {len(test_messages)} messages")
        
        # Fix tokenizer
        tokenizer = fix_tokenizer_attention_mask()
        
        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            "CohereLabs/c4ai-command-r-plus-08-2024",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… Model loaded")
        
        # Create clustering prompt
        messages_text = "\\n".join([
            f"{i+1}. {row['user']}: {str(row['content'])[:50]}"
            for i, (_, row) in enumerate(test_messages.iterrows())
        ])
        
        clustering_prompt = f"""Group these messages into topic clusters:

{messages_text}

Return JSON format with clusters."""
        
        # Apply chat template
        messages_chat = [{"role": "user", "content": clustering_prompt}]
        
        inputs = tokenizer.apply_chat_template(
            messages_chat, 
            tokenize=True, 
            add_generation_prompt=True, 
            return_tensors="pt"
        )
        
        print(f"âœ… Clustering prompt prepared: {inputs.shape}")
        
        # Generate clusters
        print("ğŸ”„ Generating clusters...")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=200,  # Reasonable size
                do_sample=True,
                temperature=0.2,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        
        print("âœ… Clustering completed!")
        print("\\nğŸ“‹ Clustering Response:")
        print("-" * 40)
        print(response)
        print("-" * 40)
        
        # Save results
        result = {
            "success": True,
            "attention_mask_fixed": True,
            "messages_analyzed": len(test_messages),
            "clusters_response": response,
            "pad_token": str(tokenizer.pad_token),
            "eos_token": str(tokenizer.eos_token)
        }
        
        os.makedirs("output", exist_ok=True)
        with open("output/cohere_fixed_attention_results.json", "w") as f:
            json.dump(result, f, indent=2)
        
        print("\\nğŸ’¾ Results saved to output/cohere_fixed_attention_results.json")
        print("\\nğŸ‰ Cohere Step 1 clustering working with fixed attention mask!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Clustering test failed: {e}")
        return False

def main():
    """Main function"""
    
    print("ğŸ”§ Cohere Attention Mask Fix")
    print("=" * 30)
    
    # Test 1: Simple generation
    if not test_fixed_generation():
        print("\\nâŒ Basic generation test failed")
        return
    
    # Test 2: Clustering
    if test_clustering_with_fixed_attention():
        print("\\nğŸ¯ SUCCESS! Attention mask issue fixed!")
        print("=" * 40)
        print("âœ… No more attention mask warnings")
        print("âœ… Generation works without hanging")
        print("âœ… Clustering produces results")
        print("âœ… Ready for Step 1 evaluation")
        
        print("\\nğŸ“ˆ Key fixes applied:")
        print("   - Set pad_token â‰  eos_token")
        print("   - Used unk_token as pad_token")
        print("   - Proper attention mask handling")
        
        print("\\nğŸš€ Next: Test with more messages (10, 20, 50, 200)")
    else:
        print("\\nâŒ Clustering test failed")

if __name__ == "__main__":
    main()
